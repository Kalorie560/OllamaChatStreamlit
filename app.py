import streamlit as st
import ollama
import time
import requests
import json
import os

st.set_page_config(
    page_title="Local LLM Chat with Ollama",
    page_icon="ðŸ¤–",
    layout="centered"
)

# Initialize session state for chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

# Function to get available models directly from Ollama API
def get_available_models():
    try:
        # Direct HTTP request to Ollama API - more reliable than the Python client
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code != 200:
                st.error(f"Ollama service returned unexpected status: {response.status_code}")
                st.info("Try running 'ollama serve' in a terminal, then restart this app")
                return []
                
            # Parse the JSON response
            data = response.json()
            models = data.get('models', [])
            
            # Get model names
            model_names = [model.get('name') for model in models if model.get('name')]
            
            if not model_names:
                # Fallback to manual command if API response is empty
                st.warning("No models found via API, trying manual method...")
                result = os.popen("ollama list").read()
                if "NAME" in result:
                    # Parse the command output
                    lines = result.strip().split('\n')[1:]  # Skip header
                    model_names = [line.split()[0] for line in lines if line.strip()]
            
            # Final check if we have models
            if not model_names:
                st.warning("No models found in Ollama")
                st.info("Please use terminal to pull models: ollama pull model_name")
                return []
                
            return model_names
                
        except requests.exceptions.ConnectionError:
            st.error("Ollama service is not running")
            st.info("Please start Ollama with 'ollama serve' command")
            return []
            
    except Exception as e:
        st.error(f"Error fetching models: {str(e)}")
        st.error("Make sure Ollama is running with 'ollama serve' command")
        
        # Last resort - hardcode the models we know exist
        st.warning("Attempting to use known models...")
        return ["deepseek-r1:8b", "qwen:0.5b"]

# Function to check if a model exists (no pulling)
def check_model(model_name):
    try:
        # First check if model_name is empty
        if not model_name:
            st.error("No model selected")
            return False
            
        # ASSUME THE MODEL EXISTS - this is a workaround for unreliable API responses
        # We know these models exist based on the command line output
        if model_name in ["deepseek-r1:8b", "qwen:0.5b"]:
            # These models are confirmed to exist via command line
            return True
            
        # Fallback to checking the API
        try:
            # Try direct HTTP request to Ollama API
            response = requests.get("http://localhost:11434/api/tags", timeout=3)
            if response.status_code == 200:
                data = response.json()
                for model in data.get('models', []):
                    if model.get('name') == model_name:
                        return True
            
            # If API check failed, try command line check
            result = os.popen(f"ollama list | grep {model_name}").read()
            if model_name in result:
                return True
            
            # If we get here, we couldn't find the model
            st.error(f"Model {model_name} not found")
            st.info(f"Please use terminal to pull it: ollama pull {model_name}")
            return False
                
        except Exception as e:
            # If there's an API error, assume Ollama is running but having API issues
            # Try direct Python client call
            try:
                ollama.show(model_name)
                return True
            except:
                # Last resort - check if we're using a known model
                if model_name in ["deepseek-r1:8b", "qwen:0.5b"]:
                    return True
                    
                st.error(f"Error verifying model {model_name}: {str(e)}")
                return False
    
    except Exception as e:
        st.error(f"Error checking models: {str(e)}")
        st.warning("Attempting to proceed anyway...")
        # Last resort - assume it works for known models
        if model_name in ["deepseek-r1:8b", "qwen:0.5b"]:
            return True
        return False

# Main app header
st.title("ðŸ¤– Local LLM Chat")
st.caption("Powered by Ollama")

# Sidebar for model info and settings
with st.sidebar:
    st.header("About")
    st.markdown("""
    This app uses:
    - **Ollama**: For running LLMs locally
    - **Streamlit**: For the UI
    
    Select a model from the dropdown and chat with it!
    
    To add more models, use the terminal command:
    ```
    ollama pull modelname
    ```
    """)
    
    st.header("Memory Settings")
    # Add memory management options
    if "memory_limit" not in st.session_state:
        st.session_state.memory_limit = "medium"
    
    memory_option = st.radio(
        "Memory Usage:",
        ["low", "medium", "high"],
        index=1,
        help="Controls how much RAM the model uses. Lower settings use less RAM but may reduce quality."
    )
    
    if memory_option != st.session_state.memory_limit:
        st.session_state.memory_limit = memory_option
        st.toast(f"Memory usage set to {memory_option}")
    
    # Maximum conversation length to prevent memory issues
    if "max_messages" not in st.session_state:
        st.session_state.max_messages = 10
    
    max_msg = st.slider(
        "Max Conversation Length:",
        min_value=5,
        max_value=20,
        value=st.session_state.max_messages,
        help="Limits how many messages are kept in history to save memory"
    )
    
    if max_msg != st.session_state.max_messages:
        st.session_state.max_messages = max_msg
        # Trim history if needed
        if len(st.session_state.messages) > max_msg*2:  # *2 because each exchange is 2 messages
            st.session_state.messages = st.session_state.messages[-max_msg*2:]
            st.toast(f"Trimmed conversation history to {max_msg} exchanges")
    
    if st.button("Clear conversation"):
        st.session_state.messages = []
        st.rerun()

# Add memory stats display
def get_memory_usage():
    try:
        import psutil
        process = psutil.Process()
        memory_info = process.memory_info()
        return {
            "rss": f"{memory_info.rss / (1024 * 1024):.1f} MB",
            "percent": f"{process.memory_percent():.1f}%"
        }
    except:
        return {"rss": "N/A", "percent": "N/A"}

# Get list of available models
available_models = get_available_models()

# Model selection in sidebar
with st.sidebar:
    st.header("Model Selection")
    
    if not available_models:
        st.error("No models available")
        st.info("Please use terminal to pull models: ollama pull model_name")
        MODEL_NAME = ""
        model_available = False
    else:
        # Default to first available model if none selected yet
        if "selected_model" not in st.session_state:
            st.session_state.selected_model = available_models[0] if available_models else ""
            
        # Model selector dropdown
        selected_model = st.selectbox(
            "Choose a model:",
            options=available_models,
            index=available_models.index(st.session_state.selected_model) if st.session_state.selected_model in available_models else 0
        )
        
        # Update the session state if model changed
        if selected_model != st.session_state.selected_model:
            st.session_state.selected_model = selected_model
            # Clear message history when switching models
            if st.session_state.messages:
                if st.checkbox("Keep conversation history when switching models?", value=False):
                    pass
                else:
                    st.session_state.messages = []
                    st.rerun()
        
        MODEL_NAME = st.session_state.selected_model
        model_available = check_model(MODEL_NAME)

# Display memory usage statistics in an auto-updating container 
with st.sidebar:
    st.header("System Stats")
    stats_container = st.empty()
    
    # Create an auto-updating function using st.empty and session state
    def update_memory_stats():
        mem_stats = get_memory_usage()
        stats_container.text(f"App Memory: {mem_stats['rss']} ({mem_stats['percent']})")
        
    # Update stats initially
    update_memory_stats()
    
    # Allow manual refresh too
    if st.button("Refresh Stats"):
        update_memory_stats()

# Display chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# Chat input
if model_available:
    prompt = st.chat_input("Ask something...")
    if prompt:
        # Add user message to chat
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)
        
        # Generate assistant response
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            
            # Stream the response
            with st.spinner("Thinking..."):
                try:
                    messages_for_api = [{"role": msg["role"], "content": msg["content"]} 
                                       for msg in st.session_state.messages]
                    
                    # Simplified and optimized memory settings
                    memory_settings = {
                        "low": {
                            "num_ctx": 2048,
                            "num_batch": 256
                        },
                        "medium": {
                            "num_ctx": 4096,
                            "num_batch": 512  
                        },
                        "high": {
                            "num_ctx": 8192,
                            "num_batch": 1024
                        }
                    }
                    
                    # Get the current model and memory setting
                    model_name = MODEL_NAME
                    current_memory = st.session_state.memory_limit
                    
                    # Display which model is being used
                    with st.status(f"Using model: {model_name}", expanded=False):
                        st.write(f"Memory setting: {current_memory}")
                        
                    # Optimized for better performance - only apply context window for high mode
                    options = {}
                    if current_memory == "high":
                        options = memory_settings["high"]
                    
                    # For better performance, don't pass empty options dictionary
                    if options:
                        response = ollama.chat(
                            model=model_name,
                            messages=messages_for_api,
                            stream=True,
                            options=options
                        )
                    else:
                        response = ollama.chat(
                            model=model_name,
                            messages=messages_for_api,
                            stream=True
                        )
                    
                    # Remove delay for faster response and update memory stats during streaming
                    update_counter = 0
                    for chunk in response:
                        if chunk.get('message', {}).get('content'):
                            content = chunk['message']['content']
                            full_response += content
                            message_placeholder.write(full_response + "â–Œ")
                            
                            # Update memory stats and display model info periodically during streaming
                            update_counter += 1
                            if update_counter % 10 == 0:  # Update every 10 chunks
                                update_memory_stats()
                    
                    message_placeholder.write(full_response)
                except Exception as e:
                    st.error(f"Error generating response: {e}")
                    full_response = f"I encountered an error: {str(e)}"
                    message_placeholder.write(full_response)
            
            # Add assistant response to chat history
            st.session_state.messages.append({"role": "assistant", "content": full_response})
else:
    st.error("Failed to load the model. Please check if Ollama is running and try again.")
